{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyCpNDSDHNv2",
        "outputId": "815e507c-4401-4c3a-d14f-8ef6e928734d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (0.1.5)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rv7fDnCqHc2F"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DiabetesBigDataAnalytics\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuPx1XfhH73j",
        "outputId": "d50a83e1-33ce-41ba-bf7e-8fa5a75e8eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-16 14:18:05--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23278 (23K) [text/plain]\n",
            "Saving to: â€˜diabetes.csvâ€™\n",
            "\n",
            "\rdiabetes.csv          0%[                    ]       0  --.-KB/s               \rdiabetes.csv        100%[===================>]  22.73K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2026-01-16 14:18:05 (16.3 MB/s) - â€˜diabetes.csvâ€™ saved [23278/23278]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv -O diabetes.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rz2fO5BLH-mx"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"diabetes.csv\", inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jk8u4xGZIBqP"
      },
      "outputs": [],
      "source": [
        "df = df.toDF(\n",
        "    \"Pregnancies\",\"Glucose\",\"BloodPressure\",\n",
        "    \"SkinThickness\",\"Insulin\",\"BMI\",\n",
        "    \"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3MJBPBNIBm4",
        "outputId": "b22c4e2b-7932-402d-cca1-7fda7b52666d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
            "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
            "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
            "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
            "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
            "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
            "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
            "only showing top 5 rows\n",
            "root\n",
            " |-- Pregnancies: integer (nullable = true)\n",
            " |-- Glucose: integer (nullable = true)\n",
            " |-- BloodPressure: integer (nullable = true)\n",
            " |-- SkinThickness: integer (nullable = true)\n",
            " |-- Insulin: integer (nullable = true)\n",
            " |-- BMI: double (nullable = true)\n",
            " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Outcome: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5)\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYz2MJYoIBHu",
        "outputId": "8b84657b-e057-417b-83be-aa1c51aa3f4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Pregnancies: integer (nullable = true)\n",
            " |-- Glucose: integer (nullable = true)\n",
            " |-- BloodPressure: integer (nullable = true)\n",
            " |-- SkinThickness: integer (nullable = true)\n",
            " |-- Insulin: integer (nullable = true)\n",
            " |-- BMI: double (nullable = true)\n",
            " |-- DiabetesPedigreeFunction: double (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Outcome: integer (nullable = true)\n",
            "\n",
            "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
            "|summary|       Pregnancies|          Glucose|     BloodPressure|     SkinThickness|           Insulin|               BMI|DiabetesPedigreeFunction|               Age|           Outcome|\n",
            "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
            "|  count|               768|              768|               768|               768|               768|               768|                     768|               768|               768|\n",
            "|   mean|3.8450520833333335|     120.89453125|       69.10546875|20.536458333333332| 79.79947916666667|31.992578124999977|      0.4718763020833327|33.240885416666664|0.3489583333333333|\n",
            "| stddev|  3.36957806269887|31.97261819513622|19.355807170644777|15.952217567727642|115.24400235133803| 7.884160320375441|       0.331328595012775|11.760231540678689| 0.476951377242799|\n",
            "|    min|                 0|                0|                 0|                 0|                 0|               0.0|                   0.078|                21|                 0|\n",
            "|    max|                17|              199|               122|                99|               846|              67.1|                    2.42|                81|                 1|\n",
            "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------------+------------------+------------------+\n",
            "\n",
            "+-------+-----+\n",
            "|Outcome|count|\n",
            "+-------+-----+\n",
            "|      1|  268|\n",
            "|      0|  500|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()\n",
        "df.describe().show()\n",
        "df.groupBy(\"Outcome\").count().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Si9UKR0QIA_V"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "for c in df.columns:\n",
        "    median = df.approxQuantile(c, [0.5], 0.01)[0]\n",
        "    df = df.withColumn(c, when(col(c).isNull(), median).otherwise(col(c)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IOWy6dubIcIx"
      },
      "outputs": [],
      "source": [
        "zero_cols = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n",
        "\n",
        "for c in zero_cols:\n",
        "    median = df.approxQuantile(c, [0.5], 0.01)[0]\n",
        "    df = df.withColumn(c, when(col(c)==0, median).otherwise(col(c)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZrZzF84cIe2n"
      },
      "outputs": [],
      "source": [
        "def cap_outliers(df, c):\n",
        "    q1 = df.approxQuantile(c, [0.25], 0.01)[0]\n",
        "    q3 = df.approxQuantile(c, [0.75], 0.01)[0]\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    return df.withColumn(\n",
        "        c,\n",
        "        when(col(c)<lower, lower)\n",
        "        .when(col(c)>upper, upper)\n",
        "        .otherwise(col(c))\n",
        "    )\n",
        "\n",
        "for c in [\"Glucose\",\"BloodPressure\",\"Insulin\",\"BMI\"]:\n",
        "    df = cap_outliers(df, c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "36hwzENlIk1X"
      },
      "outputs": [],
      "source": [
        "pdf = df.toPandas()\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "X = pdf.drop(\"Outcome\", axis=1)\n",
        "y = pdf[\"Outcome\"]\n",
        "\n",
        "X_bal, y_bal = SMOTE().fit_resample(X, y)\n",
        "\n",
        "import pandas as pd\n",
        "df = spark.createDataFrame(pd.concat([X_bal, y_bal], axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K3xoTynkIl_5"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=X.columns.tolist(),\n",
        "    outputCol=\"raw_features\"\n",
        ")\n",
        "\n",
        "df_vec = assembler.transform(df)\n",
        "\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"raw_features\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=True,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "scaler_model = scaler.fit(df_vec)\n",
        "final_df = scaler_model.transform(df_vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uV83djAFIr3U"
      },
      "outputs": [],
      "source": [
        "train, test = final_df.randomSplit([0.8, 0.2], seed=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C1A4JnMKI1Eg"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "R7--TNDwI2fU"
      },
      "outputs": [],
      "source": [
        "accuracy_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "precision_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"weightedPrecision\"\n",
        ")\n",
        "\n",
        "recall_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"weightedRecall\"\n",
        ")\n",
        "\n",
        "f1_eval = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"f1\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-2iegnMJLua",
        "outputId": "628d5c7d-8b23-4dbf-c9d3-2cff4c198a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression\n",
            "Accuracy : 0.7308\n",
            "Precision: 0.7318\n",
            "Recall   : 0.7308\n",
            "F1-Score : 0.7311\n",
            "\n",
            "Decision Tree\n",
            "Accuracy : 0.7637\n",
            "Precision: 0.7716\n",
            "Recall   : 0.7637\n",
            "F1-Score : 0.7588\n",
            "\n",
            "Random Forest\n",
            "Accuracy : 0.7912\n",
            "Precision: 0.7928\n",
            "Recall   : 0.7912\n",
            "F1-Score : 0.7895\n",
            "\n",
            "GBTClassifier\n",
            "Accuracy : 0.7692\n",
            "Precision: 0.7703\n",
            "Recall   : 0.7692\n",
            "F1-Score : 0.7674\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
        "\n",
        "# Define the models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\"),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"Outcome\", featuresCol=\"features\"),\n",
        "    \"Random Forest\": RandomForestClassifier(labelCol=\"Outcome\", featuresCol=\"features\"),\n",
        "    \"GBTClassifier\": GBTClassifier(labelCol=\"Outcome\", featuresCol=\"features\")\n",
        "}\n",
        "\n",
        "cresults = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    trained_model = model.fit(train)\n",
        "    predictions = trained_model.transform(test)\n",
        "\n",
        "    accuracy = accuracy_eval.evaluate(predictions)\n",
        "    precision = precision_eval.evaluate(predictions)\n",
        "    recall = recall_eval.evaluate(predictions)\n",
        "    f1 = f1_eval.evaluate(predictions)\n",
        "\n",
        "    cresults.append((\n",
        "        name,\n",
        "        round(accuracy, 4),\n",
        "        round(precision, 4),\n",
        "        round(recall, 4),\n",
        "        round(f1, 4)\n",
        "    ))\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"Accuracy : {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall   : {recall:.4f}\")\n",
        "    print(f\"F1-Score : {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3w34w-dJOTG",
        "outputId": "8e94fea0-8469-410b-8a28-97a0ecffc155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------+---------+------+--------+\n",
            "|Model              |Accuracy|Precision|Recall|F1-Score|\n",
            "+-------------------+--------+---------+------+--------+\n",
            "|Logistic Regression|0.7308  |0.7318   |0.7308|0.7311  |\n",
            "|Decision Tree      |0.7637  |0.7716   |0.7637|0.7588  |\n",
            "|Random Forest      |0.7912  |0.7928   |0.7912|0.7895  |\n",
            "|GBTClassifier      |0.7692  |0.7703   |0.7692|0.7674  |\n",
            "+-------------------+--------+---------+------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results_df = spark.createDataFrame(\n",
        "    cresults,\n",
        "    [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        ")\n",
        "\n",
        "results_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUsDQNYbJoBU",
        "outputId": "740f3a8c-bff2-4e28-d0de-b86e2fca09cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: Random Forest\n"
          ]
        }
      ],
      "source": [
        "best_model = results_df.orderBy(\"F1-Score\", ascending=False).first()\n",
        "print(\"Best Model:\", best_model[\"Model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "T3r4ywNzJrQh"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "gbt = GBTClassifier(labelCol=\"Outcome\")\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n",
        "    .addGrid(gbt.maxIter, [20, 50]) \\\n",
        "    .build()\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"Outcome\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "cv = CrossValidator(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3\n",
        ")\n",
        "\n",
        "cv_model = cv.fit(train)\n",
        "best_model = cv_model.bestModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "e317IInwKea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a474e050-5ec3-4657-dd81-a4d258656ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7802\n",
            "Precision: 0.7810\n",
            "Recall: 0.8283\n",
            "F1 Score: 0.7793\n",
            "Best MaxDepth: 3\n",
            "Best MaxIter: 20\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# 1ï¸âƒ£ Define the model\n",
        "gbt = GBTClassifier(labelCol=\"Outcome\")\n",
        "\n",
        "# 2ï¸âƒ£ Define a smaller hyperparameter grid for speed\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(gbt.maxDepth, [3, 5]) \\\n",
        "    .addGrid(gbt.maxIter, [20]) \\\n",
        "    .build()\n",
        "\n",
        "# 3ï¸âƒ£ Binary evaluator for TrainValidationSplit\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# 4ï¸âƒ£ TrainValidationSplit (much faster than CV)\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8,  # 80% train, 20% validation\n",
        "    parallelism=2    # adjust based on CPU cores\n",
        ")\n",
        "\n",
        "# 5ï¸âƒ£ Fit the model\n",
        "tvs_model = tvs.fit(train)\n",
        "\n",
        "# 6ï¸âƒ£ Get best model\n",
        "best_model = tvs_model.bestModel\n",
        "\n",
        "# 7ï¸âƒ£ Make predictions on test set\n",
        "predictions = best_model.transform(test)\n",
        "\n",
        "# 8ï¸âƒ£ Evaluators for metrics\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "precision_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"precisionByLabel\", metricLabel=1.0\n",
        ")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"recallByLabel\", metricLabel=1.0\n",
        ")\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"f1\"\n",
        ")\n",
        "\n",
        "# 9ï¸âƒ£ Calculate metrics\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "precision = precision_evaluator.evaluate(predictions)\n",
        "recall = recall_evaluator.evaluate(predictions)\n",
        "f1 = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "# 10ï¸âƒ£ Print metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# 11ï¸âƒ£ Optional: best hyperparameters\n",
        "print(\"Best MaxDepth:\", best_model.getOrDefault(\"maxDepth\"))\n",
        "print(\"Best MaxIter:\", best_model.getOrDefault(\"maxIter\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hNjjhTgUJtlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda2077a-f48a-497d-efcf-70c3112c466b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7802197802197802\n",
            "F1 Score: 0.7793309631544927\n"
          ]
        }
      ],
      "source": [
        "pred = best_model.transform(test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_eval.evaluate(pred))\n",
        "print(\"F1 Score:\", f1_eval.evaluate(pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NhPAsGdVMFav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1892aec9-b3c6-4906-c9d1-8fa66b82f54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7967\n",
            "Precision: 0.7818\n",
            "Recall: 0.8687\n",
            "F1 Score: 0.7948\n",
            "Best MaxDepth: 3\n",
            "Best MaxIter: 50\n",
            "Best StepSize: 0.1\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "# =======================\n",
        "# 1ï¸âƒ£ Define the model\n",
        "# =======================\n",
        "gbt = GBTClassifier(labelCol=\"Outcome\")\n",
        "\n",
        "# =======================\n",
        "# 2ï¸âƒ£ Hyperparameter grid (fast version)\n",
        "# =======================\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(gbt.maxDepth, [3, 5])       # test 2 depths\n",
        "    .addGrid(gbt.maxIter, [20, 50])      # test 2 iterations\n",
        "    .addGrid(gbt.stepSize, [0.1])        # fixed step size\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# 3ï¸âƒ£ Binary evaluator for TrainValidationSplit\n",
        "# =======================\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"Outcome\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# 4ï¸âƒ£ TrainValidationSplit\n",
        "# =======================\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8,      # 80% train, 20% validation\n",
        "    parallelism=2         # adjust based on CPU cores\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# 5ï¸âƒ£ Fit the model\n",
        "# =======================\n",
        "tvs_model = tvs.fit(train)\n",
        "\n",
        "# =======================\n",
        "# 6ï¸âƒ£ Get the best model\n",
        "# =======================\n",
        "best_model = tvs_model.bestModel\n",
        "\n",
        "# =======================\n",
        "# 7ï¸âƒ£ Make predictions on test set\n",
        "# =======================\n",
        "predictions = best_model.transform(test)\n",
        "\n",
        "# =======================\n",
        "# 8ï¸âƒ£ Evaluate metrics\n",
        "# =======================\n",
        "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ")\n",
        "precision_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"precisionByLabel\", metricLabel=1.0\n",
        ")\n",
        "recall_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"recallByLabel\", metricLabel=1.0\n",
        ")\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"f1\"\n",
        ")\n",
        "\n",
        "accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "precision = precision_evaluator.evaluate(predictions)\n",
        "recall = recall_evaluator.evaluate(predictions)\n",
        "f1 = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "# =======================\n",
        "# 9ï¸âƒ£ Print metrics\n",
        "# =======================\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# =======================\n",
        "# ðŸ”Ÿ Print best hyperparameters\n",
        "# =======================\n",
        "print(\"Best MaxDepth:\", best_model.getOrDefault(\"maxDepth\"))\n",
        "print(\"Best MaxIter:\", best_model.getOrDefault(\"maxIter\"))\n",
        "print(\"Best StepSize:\", best_model.getOrDefault(\"stepSize\"))\n",
        "\n",
        "# =======================\n",
        "# 1ï¸âƒ£1ï¸âƒ£ Optional: Save the model\n",
        "# =======================\n",
        "# best_model.save(\"best_gbt_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxyVQVbgNr_5"
      },
      "source": [
        "to improve accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r2YSbAAsNtuV"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import when\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "h7SH4Y56NvnY"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights\n",
        "total = train.count()\n",
        "pos = train.filter(\"Outcome==1\").count()\n",
        "neg = total - pos\n",
        "\n",
        "weight_for_1 = total / (2 * pos)\n",
        "weight_for_0 = total / (2 * neg)\n",
        "\n",
        "# Add weight column\n",
        "train = train.withColumn(\"classWeight\", when(train[\"Outcome\"]==1, weight_for_1).otherwise(weight_for_0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fZiP1cIQNxlu"
      },
      "outputs": [],
      "source": [
        "# GBT\n",
        "gbt = GBTClassifier(labelCol=\"Outcome\", weightCol=\"classWeight\")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(labelCol=\"Outcome\", weightCol=\"classWeight\")\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(labelCol=\"Outcome\", weightCol=\"classWeight\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jKM8e3s_N1iA"
      },
      "outputs": [],
      "source": [
        "# GBT Grid\n",
        "gbt_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(gbt.maxDepth, [3, 5, 7])\n",
        "    .addGrid(gbt.maxIter, [50, 100])\n",
        "    .addGrid(gbt.stepSize, [0.05, 0.1])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Random Forest Grid\n",
        "rf_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [50, 100])\n",
        "    .addGrid(rf.maxDepth, [5, 7])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# Logistic Regression Grid\n",
        "lr_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, [0.01, 0.1])\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
        "    .build()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "k-QarNUMN4A2"
      },
      "outputs": [],
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vSk4sDDrN39d"
      },
      "outputs": [],
      "source": [
        "def train_model(estimator, paramGrid, train):\n",
        "    tvs = TrainValidationSplit(\n",
        "        estimator=estimator,\n",
        "        estimatorParamMaps=paramGrid,\n",
        "        evaluator=evaluator,\n",
        "        trainRatio=0.8,\n",
        "        parallelism=2  # adjust based on CPU cores\n",
        "    )\n",
        "    model = tvs.fit(train)\n",
        "    return model.bestModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IvtxuoVsN6F5"
      },
      "outputs": [],
      "source": [
        "best_gbt = train_model(gbt, gbt_paramGrid, train)\n",
        "best_rf = train_model(rf, rf_paramGrid, train)\n",
        "best_lr = train_model(lr, lr_paramGrid, train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AJNeY6f8N8Nk"
      },
      "outputs": [],
      "source": [
        "predictions_gbt = best_gbt.transform(test)\n",
        "predictions_rf = best_rf.transform(test)\n",
        "predictions_lr = best_lr.transform(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(pred):\n",
        "    acc = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(pred)\n",
        "    precision = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"precisionByLabel\", metricLabel=1.0).evaluate(pred)\n",
        "    recall = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"recallByLabel\", metricLabel=1.0).evaluate(pred)\n",
        "    f1 = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(pred)\n",
        "    return acc, precision, recall, f1\n",
        "\n",
        "metrics_gbt = evaluate(predictions_gbt)\n",
        "metrics_rf = evaluate(predictions_rf)\n",
        "metrics_lr = evaluate(predictions_lr)\n",
        "\n",
        "print(\"GBT Metrics: Accuracy={:.4f}, Precision={:.4f}, Recall={:.4f}, F1={:.4f}\".format(*metrics_gbt))\n",
        "print(\"RF Metrics: Accuracy={:.4f}, Precision={:.4f}, Recall={:.4f}, F1={:.4f}\".format(*metrics_rf))\n",
        "print(\"LR Metrics: Accuracy={:.4f}, Precision={:.4f}, Recall={:.4f}, F1={:.4f}\".format(*metrics_lr))\n"
      ],
      "metadata": {
        "id": "BdwFz7WDVd9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463909e9-ca09-45b8-a29b-41822d721e59"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GBT Metrics: Accuracy=0.8132, Precision=0.7876, Recall=0.8990, F1=0.8108\n",
            "RF Metrics: Accuracy=0.8077, Precision=0.7909, Recall=0.8788, F1=0.8059\n",
            "LR Metrics: Accuracy=0.7253, Precision=0.7579, Recall=0.7273, F1=0.7257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = max(\n",
        "    [(\"GBT\", metrics_gbt[0], best_gbt),\n",
        "     (\"RF\", metrics_rf[0], best_rf),\n",
        "     (\"LR\", metrics_lr[0], best_lr)],\n",
        "    key=lambda x: x[1]\n",
        ")\n",
        "\n",
        "print(f\"Best model: {best_model_name[0]} with Accuracy: {best_model_name[1]:.4f}\")\n",
        "best_model_to_use = best_model_name[2]\n"
      ],
      "metadata": {
        "id": "X3kZlEESV615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec154a7-b58a-4c0b-f7da-bf1511a862e1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: GBT with Accuracy: 0.8132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to improve beyond 0.85"
      ],
      "metadata": {
        "id": "e9wGkqGRWER4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import when\n"
      ],
      "metadata": {
        "id": "OR-rgFPrW5fx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights\n",
        "total = train.count()\n",
        "pos = train.filter(\"Outcome==1\").count()\n",
        "neg = total - pos\n",
        "\n",
        "weight_for_1 = total / (2 * pos)\n",
        "weight_for_0 = total / (2 * neg)\n",
        "\n",
        "# Add class weight column\n",
        "train = train.withColumn(\"classWeight\", when(train[\"Outcome\"]==1, weight_for_1).otherwise(weight_for_0))\n"
      ],
      "metadata": {
        "id": "YaETLa8DW9tv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest (main model)\n",
        "rf = RandomForestClassifier(labelCol=\"Outcome\", weightCol=\"classWeight\", seed=42)\n",
        "\n",
        "# GBT (optional for ensemble)\n",
        "gbt = GBTClassifier(labelCol=\"Outcome\", weightCol=\"classWeight\", seed=42)\n"
      ],
      "metadata": {
        "id": "s7jYlBKZXBnu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RF grid\n",
        "rf_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [100, 150])\n",
        "    .addGrid(rf.maxDepth, [5, 7, 10])\n",
        "    .addGrid(rf.subsamplingRate, [0.8, 1.0])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "# GBT grid (optional)\n",
        "gbt_paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(gbt.maxDepth, [3, 5, 7])\n",
        "    .addGrid(gbt.maxIter, [50, 100])\n",
        "    .addGrid(gbt.stepSize, [0.05, 0.1])\n",
        "    .build()\n",
        ")\n"
      ],
      "metadata": {
        "id": "ebuXLRN2XCzg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n"
      ],
      "metadata": {
        "id": "ER8itBbYXGZ1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(estimator, paramGrid, train_df):\n",
        "    tvs = TrainValidationSplit(\n",
        "        estimator=estimator,\n",
        "        estimatorParamMaps=paramGrid,\n",
        "        evaluator=evaluator,\n",
        "        trainRatio=0.8,\n",
        "        parallelism=4  # use all CPU cores\n",
        "    )\n",
        "    best_model = tvs.fit(train_df).bestModel\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "A1aZ-lmSXGuh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf = train_model(rf, rf_paramGrid, train)\n",
        "best_gbt = train_model(gbt, gbt_paramGrid, train)  # optional for ensemble\n"
      ],
      "metadata": {
        "id": "bGUK3c9_XIek"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.cache()  # cache for faster repeated access\n",
        "\n",
        "pred_rf = best_rf.transform(test)\n",
        "pred_gbt = best_gbt.transform(test)\n"
      ],
      "metadata": {
        "id": "LIxdtgJuXKq6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Average probabilities of class 1\n",
        "avg_prob_udf = udf(lambda rf_prob, gbt_prob: (rf_prob[1] + gbt_prob[1])/2, DoubleType())\n",
        "\n",
        "ensemble_df = pred_rf.alias(\"rf\").join(\n",
        "    pred_gbt.select(col(\"Outcome\"), col(\"probability\").alias(\"gbt_probability\")).alias(\"gbt\"),\n",
        "    on=\"Outcome\",\n",
        "    how=\"inner\"\n",
        ") \\\n",
        ".withColumn(\"avg_prob\", avg_prob_udf(col(\"rf.probability\"), col(\"gbt.gbt_probability\"))) \\\n",
        ".withColumn(\"prediction\", (col(\"avg_prob\") > 0.5).cast(\"double\"))"
      ],
      "metadata": {
        "id": "BBVxr7ORY0WM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(pred_df):\n",
        "    acc = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(pred_df)\n",
        "    precision = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"precisionByLabel\", metricLabel=1.0).evaluate(pred_df)\n",
        "    recall = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"recallByLabel\", metricLabel=1.0).evaluate(pred_df)\n",
        "    f1 = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(pred_df)\n",
        "    return acc, precision, recall, f1\n",
        "\n",
        "accuracy, precision, recall, f1 = evaluate(pred_rf)  # or ensemble_df for ensemble\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "FCpZzOH1Y3o-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8fc26a-6934-4520-8c5d-22b2b0a7ef1f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy: 0.8297\n",
            "Precision: 0.8036, Recall: 0.9091, F1: 0.8277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"diabetes.csv\", header=None)\n",
        "\n",
        "# Define columns for the dataset\n",
        "column_names = [\n",
        "    \"Pregnancies\", \"Glucose\", \"BloodPressure\",\n",
        "    \"SkinThickness\", \"Insulin\", \"BMI\",\n",
        "    \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"\n",
        "]\n",
        "df.columns = column_names\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(\"Outcome\", axis=1)\n",
        "y = df[\"Outcome\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Random Forest\n",
        "best_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaupTh3JXI3r",
        "outputId": "711f2d04-6072-4f58-ea56-551617dfe74e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.7597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"best_diabetes_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "print(\"Best model saved as best_diabetes_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX7uZPBAX5Es",
        "outputId": "44055dbb-dc3c-45a6-e4db-48cde96dff13"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved as best_diabetes_model.pkl\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}